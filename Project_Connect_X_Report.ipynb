{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Connect X - Report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Report - Connect X"
      ],
      "metadata": {
        "id": "0W0JrlPWZkAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authors: Anay Bhat, Brandon Lam, Jhon Garcia\n"
      ],
      "metadata": {
        "id": "LEFm50bSZmwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. The Problem, approach and results\n",
        "\n",
        "- Connect X is a Kaggle competition. Competitions have been mostly focused on supervised machine learning. The field has grown and is it certain to continue to provide the data science community with cutting-edge opportunities to challenge themselves and grow their skills.\n",
        "\n",
        "- In this game, your objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent. When it's your turn, you “drop” one of your checkers into one of the columns at the top of the board. Then, let your opponent take their turn. This means each move may be trying to either win for you or trying to stop your opponent from winning. The default number is four-in-a-row, but we’ll have other options to come soon.\n",
        "\n",
        "- In order to create another agent, the agent function should be fully encapsulated. In other words, it should have no external dependencies: all of the imports and helper functions need to be included.\n",
        "\n",
        "- Some of the key results are to finish the code and be able to submit our python file to the competition to be able to participate in the competition. Aside from that the learning experience with Kaggle environments which are often used in data science. \n"
      ],
      "metadata": {
        "id": "I1co5MANZrIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The importance of our problem\n",
        "\n",
        "- In this specific competition, we are tailored to create an agent \n",
        "that is able to play Connect X against the machine or a random. There are also rules that must be followed in this competition.\n",
        "\n",
        "- The importance of these competitions is crucial. Reinforcement learning is clearly a crucial piece in the next wave of data science learning. Simulation Competitions will provide the opportunity for Kagglers to practice and hone this burgeoning skill. This competition is a low-stakes, trial-run introduction.\n",
        "\n",
        "- Reinforcement learning helps determine if an algorithm is producing a correct right answer or a reward indicating it was a good decision. RL depends on cooperation between an AI framework and its current circumstance.  An algorithm receives a numerical score based on its outcome and then the positive behaviors are “reinforced” to refine the algorithm over time. In recent years, RL has been behind super-human performance on GO, Atari games, and many other applications.\n",
        "\n",
        "- Envision preparing an AI specialist to exchange stocks. One choice is to furnish the framework with numerous instances of good systems - i.e., marked data regardless of whether to sell a specific stock at a specific time. This is the notably directed learning worldview. Since the specialist is attempting to impersonate great methodologies, it can't beat them. How might we observe techniques that beat the master? The response is RL.\n",
        "\n",
        "- Once our agent is created, we accumulate data so that way we can see how our agent performs whether it is winning or losing and what’s the success rate. We can also compare against the data on the Kaggle website to see how we are performing and make improvements to our agent.\n",
        "\n",
        "- For the end result, An Agent should return which column to place a checker in. The column is an integer: [0, configuration. columns), and represents the columns going left to right. The row is an integer: [0, configuration. rows), and represents the rows going top to bottom. Our agent objective is to get a certain number of your checkers in a row horizontally, vertically, or diagonally on the game board before your opponent.\n"
      ],
      "metadata": {
        "id": "1EJ9hPLqZwy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Related Work\n",
        "\n",
        "- There are a few works that we have referenced in the creation of our game because this is a well researched problem there was lots of resources online that could be browsed to learn about different ways people have approached this issues in an effort to solve them. For example https://www.youtube.com/watch?v=MMLtza3CZFM, is a video that uses the min max algorithm for finding proper placements of pieces. It stores a record of the best places to place a piece by assigning each space a value depending on the board state which updates every time a piece is place on the board by a player or ai. Our approach borrowed the logic of choosing spaces because using a data structure to help store values relative to the space is a good idea. Though how many points are distrubuted across the spaces is something that is always up for change because of the fluctuating board space.\n",
        "\n",
        "- On the kaggle website itself there is a tutorial on how to make a rule abiding board and agent which we have used in our code as well. There are also other codes of other people who have attempted the competition by making the most efficient ai player, which were good to reference because we didn't know there was such a plethora of ways to approach the issues. Though most people seemed to trend towards a look ahead method or the AB pruning method, I don't personally know why but it seems to be the most effective so far.\n",
        "\n",
        "- There have been several attempts to try to create the “perfect” Connect 4 bot which wins most of the times it plays in the quickest time possible. One such application is Gilles Vandewiele’s article published in “Towards Data Science” called “Creating the (nearly) perfect connect-four bot with limited move time and file size.” In this article, Vandeweile discusses the intricate process him and his partner Jeroen van der Hooft use to simulate the board set up, the possible moves the bot can make in a given situation, and how the bot determines the best possible move for this situation. The process involves representing each possible game state (or game board configuration) uniquely in the form of two bitstrings, which can easily be converted to an integer: one bitstring representing the positions of the tokens of a player (position)one bitstring representing the positions of both players (mask). The board needs to be converted to a corresponding bitmap, to allow the use of numpy arrays as an input parameter for the board. Then, using the position bitstring, you can check whether four tokens are connected using bitwise shift and comparison operations. \n",
        "\n",
        "- To create a move operation, you change the bitmaps according to a move made by a player. This all leads to the point where decision trees come into play. They construct a game tree in which each node represents a possible game state. If a state is game-ending (four tokens connected or board is full), it is a leaf node. Each leaf node is awarded a certain score and not further expanded. However, there are way too many options as to how the board can be configured. Therefore, we assume that they are facing the best possible player and they are making the best possible move everytime. After defining a scoring system, they apply an algorithm called the minimax algorithm.  If the internal node is on an odd depth, we take the minimum value of the children’s values (as an opponent, they want to make the final score as negative as possible, since they want to win). If the internal node is on an even depth, we take the maximum value of the children's values (they want their score to be as positive as possible). This helps them find the best move. To help them find the best move fastest, they need to “prune” the tree, since they don’t need to traverse the whole tree everytime to find the optimal path. This pruning is called alpha-beta pruning alpha, where alpha is defined as the current best score on the path to the root by the maximizer (us), and beta is defined as the current best score on path to the root by minimizer (opponent). The alpha and beta variables are updated as the tree is traversed, and as values that are higher than beta or lower than alpha are found, the subtree gets discarded since the best path doesn’t involve these nodes. These optimizations allow for a faster, more efficient solution to the Connect 4 problem."
      ],
      "metadata": {
        "id": "rLgtWOj8aKs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data\n",
        "\n",
        "- Since our goal is to try to solve the Connect 4 problem as quickly and efficiently as possible, we want to be able to simulate games against other opponents and compare win rates. The majority of this project involves testing our agent against other agents in matches of Connect 4, making sure our agent makes the best moves in a sufficient amount of time, and then comparing our winrate score to the winrate scores of other agents. Other people have developed agents on the Kaggle page to solve the Connect 4 problem, and their agents provide a data set for us to test our agent against. There is also a Kaggle leaderboard that displays the top dynamic scores for a series of participants whose agents have played against other agents. The higher the score, the better the win rate for the agent. Using a similar method of calculation, we can use this dynamic data to determine our score and compare it to the win rates on the leaderboard. \n",
        "\n",
        "- There are also multiple tutorials and starter code blocks for the Connect 4 Problem provided, and this data can provide a good starting point to build our approach. This data is generated from multiple online Kaggle contributors and participants, who post their data on the public forum site for Connect 4. In total, there are 214 teams in this Kaggle Competition right now; this means there are 214 agents to test from and 214 scores to compare with. There was no need for any special treatment on this data because the raw data generated from comparison with the data from the Kaggle website is what we are looking for to compare our agent with to determine if ours functions up to par."
      ],
      "metadata": {
        "id": "TYGv5kUXaQNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Methods\n",
        "\n",
        "- We decided to implement a decision tree based-method, using similar techniques such as the minimax algorithm (to determine the best move in a given situation) and alpha beta pruning (to help the agent find the move in the least time possible). However, we thought to ourselves: are there optimiaztions we can make to this method to make it even better? One of the theories we discussed was perhaps improving the minimax method used in this experiment. We realized that each single move causes a new child node that has to be evaluated recursively. Therefore, a smaller number of possible moves for each board leads to less nodes and evaluations in the game tree. That means we can remove unnecessary and irrelevant moves.\n",
        "\n",
        "- For example, in Connect-Four, we don’t want to play any moves that will definitely not result in a row of 4 in future. Another way to improve the minimax method is to check before evaluating all possible moves if any of them will result in an instant win for the current player. If that’s the case, we candirectly return this move and you don’t have to evaluate the other ones. As for the alpha-beta pruning, to optimize this, we can try to pre-sort the list of possible moves starting with the best ones. In this case, there’s a higher chance of cutting of nodes within the game tree using alpha-betapruning since the algorithm starts with a high alpha or low beta value at the beginning of each level. These small optimizations we can make will help take an already efficient solution and make it even faster, which is what we are looking for with our agent. One of the only ways we brainstormed how to approach this problem was by using a neural network. However, one of the biggest reasons why we didn't utilize a neural network is the issue of overfitting. \n",
        "\n",
        "- Over-fitting is a common problem in machine learning systems, it occurs when a model adapts to perform well on the data that is being used to train it, but fails to generalize and performs poorly on data that it was not exposed to during training. In this case, the strategies which the network learn during the later part of its self-play training would give good results in its initial tests, but were not effective when tested against an opponent that played in a different style. To solve this, you could create multiple neural networks and have them play each other (because they each learn independently) and hope that the differences in their playing styles would force them to develop more general strategies that would be effective against a variety of different opponents; but this is all too much unnecessary work. This why we avoided using a neural network to simulate this problem, and instead decided to utilize a decision tree approach; despite the fact that it is prone to overfitting, we can utilize the alpha-beta pruning method to reduce thism and apply the simple optimizations above to improve the time and produce a high win rate. "
      ],
      "metadata": {
        "id": "uYGjtcLlaS1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Experiments\n",
        "\n",
        "- To start the experiment, we first had to see what kind of approach we wanted to take to start tackling the issue. The very first approach we used was the simple random agent, where the agent would just place the piece in a random spot based on a number generated from the random function. Which while isn't the best true random function was sufficient for our task. This approach helped show us that the agent worked and that results could be taken from the runs such as the wins and losses. While useful in helping us get started the training and performance of the random agent was naturally very low, though there were some odd outliers in wintime against different agents. Of course against agents that actually tried to play it had a significantly lower win rate but against another random agent it was close to a coin flip when run multiple times.\n",
        "\n",
        "- After the first approach we started looking at other feasible approaches to creating the agent. There are many helpful code blocks from other teams that really let us see into their mindset of how they crafted their agents to tackle the connectX simulations. There were many approaches other teams took to make their agents such as a look ahead agent, alpha-beta pruning agent and a minmax agent were among the most common approaches to this competition. There was among the agents, a few game ai reinforcement learning algorithms for the agent, which while great to look over and read didn't really fit our needs. So in the end we used a version of the min max method with alpha and beta pruning for our project because of it fit the needs of our agent. Such as trying to reduce the overfitting that would come with the decision tree approach.\n",
        "\n",
        "- From the experiment with the agent now decided, we saw that we were average to slightly below average when it came to comparing with others on the leaderboard. The other teams especially the top ones had very refined methods of tackling connect but while we weren't the best we had decent results from the training of our agent against other agents. The most skewed one would be from facing the random agent defined in kaggle because that one doesn't really try and can just get really lucky once in a while. While against other agents it was a bit closer against some of the normal agents and poorer against the people at the top of the leaderboard."
      ],
      "metadata": {
        "id": "azsmAqp3aUmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Conclusion\n",
        "\n",
        "- There appear to be many people who utilize the same approach to solving the Connect problem: they utilize a decision tree, the minimax method, and alpha-beta pruning to determine the best moves in the quickest amount of time.\n",
        "\n",
        "- We have learned to work with the kaggle environment that we had never used before as well as trying to do one of their competitions. It is safe to say that their description and documentation has helped us along the process of completing in creating our agent.\n",
        "\n",
        "- As stated in the Kaggle competition description, this project was called Connect X, but there were plans in the future to possibly expand this project into a greater scope, such as Connect 5 or Connect 6. We can apply concepts such as alpha-beta pruning and minimax algorithm from our Connect 4 application into the Connect 5 and Connect 6 applications. The concept of selecting the best move by placing a certain tile in a row/column combination can also be applied to another game similar to Connect 4: tic tac toe. The objective of getting 3 in a row is slightly simpler than that of Connect 4, but the concept of developing a robot that never loses in tic tac toe (i.e always draws or wins) has parallels to our application used in this Connect experiment. By identifying the best strategies and moves, we can create a smaller game tree (a decision tree of all the possible outcomes) and utilize similar strategies to eliminate the choices that would result in a loss and therefore select the best row/column coordinate to put their mark in. "
      ],
      "metadata": {
        "id": "0SVcWjzpaWwL"
      }
    }
  ]
}